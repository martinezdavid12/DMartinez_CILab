{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import PyTorch and matplotlib\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "import imageio.v3 as iio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from pthflops import count_ops\n",
    "\n",
    "vRead = iio.imread(\"c_elegans.mp4\")\n",
    "video = np.array(vRead)\n",
    "\n",
    "#Check PyTorch version\n",
    "torch.__version__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SET GPU if possible\n",
    "run in shell: \n",
    "CUDA_VISIBLE_DEVICES= {gpu#/#s}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cuda available: \", torch.cuda.is_available())\n",
    "if(torch.cuda.is_available()):\n",
    "\n",
    "    print(\"Is cuDNN version:\", torch.backends.cudnn.version())\n",
    "\n",
    "    print(\"cuDNN enabled: \", torch.backends.cudnn.enabled)\n",
    "\n",
    "    print(\"Device count: \", torch.cuda.device_count())\n",
    "\n",
    "    print(\"Current device: \", torch.cuda.current_device())\n",
    "\n",
    "    print(\"Device name: \", torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "#Setup device agnostic code (i.e use GPU if possible)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "gpuNum = 1\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Video Metadata\n",
    "import imageio.v3 as iio\n",
    "props = iio.improps(\"c_elegans.mp4\")\n",
    "print(\"Shape (frames, w, h, RGB): \\n\" + str(props.shape))\n",
    "print(props.dtype)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding Image as a Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input grid  (width_px, height_ px)\n",
    "grid = torch.empty(props.shape[1], props.shape[2], 3).to(device)\n",
    "\n",
    "# Create image tensor\n",
    "frame = 0\n",
    "image = torch.tensor(video[frame]).to(device)\n",
    "\n",
    "image.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test original identity output\n",
    "plt.imshow(image.cpu())\n",
    "plt.axis(False)\n",
    "plt.title(\"Frame: \" + str(frame));"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class SingleImageDataset(Dataset):\n",
    "    def __init__(self, image, transform=None, target_transform=None):\n",
    "        self.image = image\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "    def __len__(self):\n",
    "        return int(image.shape[0]) * int(image.shape[1])\n",
    "    def __getitem__(self, idx):\n",
    "        row = idx // int(image.shape[1])\n",
    "        col = idx % int(image.shape[1])\n",
    "        pixel = image[row][col]\n",
    "        #label = pixel\n",
    "        return row, col, pixel\n",
    "training_data = SingleImageDataset(image)\n",
    "#testing_data = None\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=32, shuffle=True)\n",
    "#train_dataloader = DataLoader(testing_data, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test Dataloader\n",
    "testGrid = torch.empty(322, 344, 3).cpu()\n",
    "for batch in iter(train_dataloader):\n",
    "    print(batch)\n",
    "    \n",
    "plt.imshow(testGrid.cpu())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "#from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "#Use seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "#Multilayer Percepetron Model \n",
    "class MLP(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_shape: int,\n",
    "                 hidden_units: int,\n",
    "                 output_shape: int):\n",
    "        super().__init__()\n",
    "        self.layer_stack = nn.Sequential(\n",
    "        nn.Linear(input_shape, hidden_units),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_units, output_shape),\n",
    "        )\n",
    "    #forward reconstruction\n",
    "    def forward(self, X):\n",
    "        return self.layer_stack(X.to(device))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instance of Model (for selected frame/image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an Instance and set loss function & optimizer\n",
    "model_0 = MLP(input_shape=3, \n",
    "              hidden_units=128, \n",
    "              output_shape=3).to(device)\n",
    "\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(params=model_0.parameters(),\n",
    "                             lr=0.001)\n",
    "#list(model_0.parameters())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Size of Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check model size by summing parameters and state_dict\n",
    "params_size = 0\n",
    "for param in model_0.parameters():\n",
    "    params_size += param.nelement() * param.element_size()\n",
    "buffer_size = 0\n",
    "for buffer in model_0.buffers():\n",
    "    buffer_size += buffer.nelement() * buffer.element_size()\n",
    "\n",
    "size_all_mb = (params_size + buffer_size) / 1024**2\n",
    "imageSize = 8 * image.shape[0] * image.shape[1] * image.shape[2]\n",
    "imageSizeMB = imageSize / (10**6)\n",
    "perDecrease = (imageSizeMB - size_all_mb) / imageSizeMB\n",
    "perDecrease *= 100\n",
    "print('original image size(no compression): {:.3f}MB'.format(imageSizeMB))\n",
    "print('model size: {:.3f}MB'.format(size_all_mb))\n",
    "print('Percent decrease in memory size: {:.3f}%'.format(perDecrease))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
